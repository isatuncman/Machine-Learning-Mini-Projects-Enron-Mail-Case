{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Fraud From Enron Email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this study is to identify  the Enron Employees who has involved fraud incident. The machine learning  algoritms are used to best classify the employees based on the features extracted from their emails. Supervison Learning methods are  very useful to  classify  data  into distinct classses based on the  features. I will try to  find best classification model using the given features and alternative classification algorithms .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this study, a POI (person of interest) is operationally defined as a person  who:\n",
    "* was  indicted\n",
    "* settled without admitting guilt\n",
    "* testified  in exchange for immunity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q1) How many data points (people) are in the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of employees:  146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q2) How many POIs are there in dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of poi's: 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q3) For each person, how many features are available?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There  exist 21  features in the dataset.  Financial features: 14, Email features: 6, POI label: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* financial features: ['salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', 'director_fees'] (all units are in US dollars)\n",
    "* email features: ['to_messages', 'email_address', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi'] (units are generally number of emails messages; notable exception is ‘email_address’, which is a text string)\n",
    "* POI label: [‘poi’] (boolean, represented as integer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q4)  Which fields include missing values in the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Missing fields and corresponding frequencies are given as below:\n",
    "* 'salary': 51 \n",
    "* 'to_messages': 60\n",
    "* 'deferral_payments': 107\n",
    "* 'total_payments': 21\n",
    "* 'long_term_incentive': 80 \n",
    "* 'loan_advances': 142 \n",
    "* 'bonus': 64\n",
    "* 'restricted_stock': 36 \n",
    "* 'restricted_stock_deferred': 128 \n",
    "* 'total_stock_value': 20\n",
    "* 'shared_receipt_with_poi': 60 \n",
    "* 'from_poi_to_this_person': 60\n",
    "* 'exercised_stock_options': 44\n",
    "* 'from_messages': 60\n",
    "* 'other': 53 \n",
    "* 'from_this_person_to_poi': 60\n",
    "* 'deferred_income': 97 \n",
    "* 'expenses': 51\n",
    "* 'email_address': 35 \n",
    "* 'director_fees': 129"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing features  are cleaned by the  feature_format function . Therefore, I did not make any addditional work to change the  feature_format function. While creating new features, I have  excluded  NaN fields in the dataset from division operations.\n",
    "We can also imput average value of the field for each missing data as a rule of thumb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q5)  What are the outliers in the dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outliers are 'TOTAL' and  'THE TRAVEL AGENCY IN THE PARK' .  The TOTAL is a spreadsheet quirk and THE TRAVEL AGENCY IN THE PARK is an important element but not a real person we are interested. So, Both of them were cleaned.  LAY KENNETH L and  SKILLING JEFFREY K are also outliers due to their  huge salaries and bonuses but  they are valid  data points. Therefore, I kept them in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q1)  What features did you end up using in your POI identifier, and what selection process did you use to pick them? \n",
    "To select features, I have used SelectKBest feature selection method.  This method  is a Univariate feature selection method works by selecting the best features based on univariate statistical tests. I have used all the features other than 'email_adress'  in  SKB. \n",
    "To find the best number of features (k) value, I have fed  all k alternatives (1-23) into the GridSearchCV  as a parameter. Therefore, for each algorithm , the best number of features were selected in the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q2)  Did you have to do any scaling? Why or why not?\n",
    "I have used scaling (minmax scaler) in the pipeline because  some of my algorithms (eg: KNeighborsClassifier) are distance based algorithms. To use these algorithms effectively, I have to use  scaling.\n",
    "I have used scaling  in the pipeline because if I do scaling before pipeline, I would be using information in the training data to perform scaling on the test data .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q3) As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it.\n",
    "I have created 3 new features to be used in the model:\n",
    "* from_this_person_fraction: This is the fraction of email messages a person sent to POIs.  It shows the relationship between POIs.\n",
    "* from_poi_to_this_person_fraction: This is the fraction of email messages a person recieved from POIs.  This is also an important  factor to find the relationship  between POIs.\n",
    "\n",
    "The rationale behind selecting these  features is that I hypothesize that POIs will have significantly higher  interaction than POI - non-POIs and Non-POIs have among each other.\n",
    "I have also created  one more feature:\n",
    "* salary/bonus ratio:  The rationale behind selecting this feature is that POIs may get significantly higher  bonus than the other people performing similar duties in the company.\n",
    "\n",
    "In the final best model the __'from_poi_to_this_person_fraction'__ feature (score: 3.13 an p value: '0.0791') has been selected. The performance metrics are:\n",
    "Accuracy: 0.85180 Precision: 0.44394 Recall: 0.44150 F1: 0.44272  \n",
    "\n",
    "If I exclude these variables from the final model, I obtain:  \n",
    "Accuracy: 0.83327\tPrecision: 0.35291\tRecall: 0.30050\tF1: 0.32460\n",
    "\n",
    "Including __from_poi_to_this_person_fraction__  and __from_this_person_fraction__ increases recall, precision, and F1 score significantly when two models are compared.\n",
    "\n",
    "* Why each k level has been selected? \n",
    "GridSearchCV selects best k value from 'SKB k': [2,5,10,20] values. For naive bayes algorithm, I fed the whole possible k values into the GridSearchCV and best k value was found to be 5. However, in Decision Tree algorithm, the classsifier performance decreases exponentially when a new parameter level is included. Therefore, I have chosen k = 2 (smaller than Naive bayes used), k = 5 (equal to Naive Bayes best k) , k = 10, and k = 20 (as an extreme).  \n",
    "when k = 2:  F1: 0.24816,    \n",
    "when k = 5:  F1: 0.21612,    \n",
    "when k = 10: F1: 0.41032,  \n",
    "when k = 20: F1: 0.44444,  \n",
    "\n",
    "\n",
    "Because the time performance of DT algorithm is also important, I did not include all possible k values as I did in Naive bayes. Here, I used Naive Bayes as a base for further complex algorithms. The best k is found to be at k=20. However, there exist little difference between k =10 and k =20. Therefore, we may also choose k =20 for the sake of simplicity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q4) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.\n",
    "\n",
    "I have used  SelectKBest Method for each method using pipeline and GridSearchCV.  For each method, all alternatives have been  tested by the GridSearchCV. I have used 3 algorithms: NaiveBayes, KNeighbor , and DecisionTree.\n",
    "* NaiveBayes: 5 features have been used (k: 5) in the best model. The features, their scores, and p values are given as below:  \n",
    " ('salary', '18.29', '0.0000'),   \n",
    " ('bonus', '20.79', '0.0000'),   \n",
    " ('total_stock_value', '24.18', '0.0000'),   \n",
    " ('exercised_stock_options', '24.82', '0.0000'),   \n",
    " ('from_this_person_fraction', '16.41', '0.0001') \n",
    "   \n",
    "     \n",
    " \n",
    " * KNeighborsClassifier: 3 features have been used (k: 3) in the best model. The features, their scores and p values are given as below:  \n",
    " ('bonus', '20.79', '0.0000'),  \n",
    " ('total_stock_value', '24.18', '0.0000'),  \n",
    " ('exercised_stock_options', '24.82', '0.0000') \n",
    " \n",
    " * Decison Tree Classifier: 20 features have been used (k:20) in the best model. The features, their scores and p values are given as below:  \n",
    " ('salary', '18.29', '0.0000'),  \n",
    " ('deferral_payments', '0.22', '0.6363'),  \n",
    " ('total_payments', '8.77', '0.0036'),  \n",
    " ('loan_advances', '7.18', '0.0082'),  \n",
    " ('bonus', '20.79', '0.0000'),  \n",
    " ('deferred_income', '11.46', '0.0009'),  \n",
    " ('total_stock_value', '24.18', '0.0000'),  \n",
    " ('expenses', '6.09', '0.0148'),  \n",
    " ('exercised_stock_options', '24.82', '0.0000'),  \n",
    " ('other', '4.19', '0.0426'),  \n",
    " ('long_term_incentive', '9.92', '0.0020'),  \n",
    " ('restricted_stock', '9.21', '0.0029'),  \n",
    " ('director_fees', '2.13', '0.1470'),  \n",
    " ('to_messages', '1.65', '0.2016'),  \n",
    " ('from_poi_to_this_person', '5.24', '0.0235'),  \n",
    " ('from_messages', '0.17', '0.6810'),  \n",
    " ('from_this_person_to_poi', '2.38', '0.1249'),  \n",
    " ('shared_receipt_with_poi', '8.59', '0.0039'),  \n",
    " ('from_this_person_fraction', '16.41', '0.0001'),  \n",
    " ('from_poi_to_this_person_fraction', '3.13', '0.0791') \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have used three algorithms: NaiveBayes, Kneighbor, and Decision Tree algorithms. The best model is found to be Decison Tree algorithm in terms of Accuracy, Recall, Precision and F1 score. The model performed significantly better especially in Recall value.  The performance metrics are explained below in detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### q1) What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm?\n",
    "The aim of parameter tuning is to find best parameter combination for a given model that gives the best solution. The 'best solution' is relative and varies according to the expectations and risks to be taken. If the parameters are not tuned well, the performance may decrease, the model may ovefit or underfit.\n",
    "To tune parameters, I have used GridSearchCV method which makes an exhaustive search over specified parameter values for an estimator. GridSearchCV gave different combinations for each model. For this, I have specified alternative ranges for number of features and other algorithm-specific parameters.\n",
    "* Naive Bayes: I only tuned number of features(SKB k) parameter.\n",
    "    \n",
    "      \n",
    "* KNeighborsClassifier: I have tuned below parameters withinn given ranges.\n",
    "    \"k\": range(1,10),  \n",
    "    \"n_neighbors\": [2,3,4,5,6,8,10],  \n",
    "    \"weights\": [\"uniform\", \"distance\"],  \n",
    "    \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"] \n",
    "    \n",
    "    \n",
    "      \n",
    "* DecisionTreeClassifier: I have tuned below parameters withinn given ranges.  \n",
    "    'k': [1,2,3,4,5,10,20]  \n",
    "    'criterion': ['gini', 'entropy']  \n",
    "    'min_samples_split': [2, 10, 20]  \n",
    "    'max_depth': [None, 2, 5, 10]  \n",
    "    'min_samples_leaf': [1, 5, 10]  \n",
    "    'max_leaf_nodes': [None, 5, 10, 20]  \n",
    "    \n",
    "In KNN and DT, I have tuned the algorithms for a few alternatives representing a large range due to performance issues. For instance, if all k ranges were used, then the performance would decrease exponentially.\n",
    "\n",
    "* The best DecisionTreeClassifier parameters were found to be:\n",
    "'max_depth': 2, 'criterion': 'entropy', 'k': 20, 'min_samples_split': 20, 'min_samples_leaf': 1, 'max_leaf_nodes': 10}\n",
    "\n",
    "* The best KNeighborsClassifier parameters were found to be:\n",
    "'algorithm': 'auto', 'weights': 'distance', 'n_neighbors': 4, 'k': 3}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric items: “discuss validation”, “validation strategy”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation is a process of evaluating a performance of trained model on a test dataset. Validation gives estimate of perfomance on an independent dataset and serves as check on overfitting. The common mistake is testing the trained model on the training dataset and 'overfitting'. Overfitted models makes low quality estimations of independent sets although they seem to be an accurate model on the training set.\n",
    "\n",
    "Due to imbalanced (POI(128) - Non POI (18) imbalance) and small-sized data, I have used stratified shuffle split cross validation. The ShuffleSplit iterator generates a user defined number of independent train / test dataset splits. Samples are first shuffled and then split into a pair of train and test sets.\n",
    "I have fed shuffle split cross validation in the GridSearchCV using 100 splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy = (True positives + True negatives) / Total predictions\n",
    "Recall = True positives / (TruePositives + False Negatives)  \n",
    "Precision = True positives / (True Positives + False Positives)\n",
    "\n",
    "F1 score conveys the balance between the precision and the recall.\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "A low recall indicates many False Negatives and a low precision indicates a large number of False Positives.\n",
    "\n",
    "\n",
    "* Decision Tree: \n",
    "  Accuracy: 0.85180\tPrecision: 0.44394\tRecall: 0.44150\tF1: 0.44272  \n",
    "  Total predictions: 15000\tTrue positives:  883 False positives:  921\tFalse negatives: 1117\tTrue negatives: 12079\n",
    "  \n",
    "  \n",
    "* Naive Bayes: \n",
    "  Accuracy: 0.85033\tPrecision: 0.42562\tRecall: 0.35050\tF1: 0.38443\n",
    "  Total predictions: 15000 True positives:  701 False positives:  946\tFalse negatives: 1299\tTrue negatives: 12054\n",
    "  \n",
    "* KNeighborsClassifier:\n",
    "  Accuracy: 0.85413\tPrecision: 0.42645\tRecall: 0.27250\tF1: 0.33252\n",
    "  Total predictions: 15000\tTrue positives:  545 False positives:  733\tFalse negatives: 1455\tTrue negatives: 12267\n",
    "\n",
    "In imbalanced datasets, acuracy can be biased. In Enron dataset prediction, if all of the predictions were negative (the model could not find any of the POIs) we would obtain (12079 + 921)/15000 = 86% accuracy. Therefore, we should use Recall, Precision and F1 values to evaluate the performance of the model.\n",
    "\n",
    "Recall is the model's capability to predict POIs from all real POIs in Enron dataset. We obtained the best recall value in Decision Tree Algorithm which performed significantly better than the others.\n",
    "\n",
    "Precision is the model's capability of predicting real POIs from all POI predictions. Higher precision means that we have less false alarms which means a small number of real Non-POIs are predicted as POIs. \n",
    "\n",
    "For this dataset, we should have more more flexibility in recall than precision because we would not want any innocent people to be punished as POIs.\n",
    "\n",
    "The Decision Tree Algorithm is able to find 45% of POIs from Enron Dataset (recall) and 44% of the total POI predictions are true POIs.\n",
    "\n",
    "The F1 score is relatively low. However, for a starting point, to be able to predict around 45% of the POIs is a significant contribution for further analysis.\n",
    "\n",
    "Although DT algorithm outperforms the others, a simpler algorithm (Naive Bayes here) using fewer features and much  less time would be a good alternative.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
