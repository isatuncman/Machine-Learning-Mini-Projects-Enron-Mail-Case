{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Fraud From Enron Email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this study is to identify  the Enron Employees who has involved fraud incident. The machine learning  algoritms are used to best classify the employees based on the features extracted from their emails. Supervison Learning methods are  very useful to  classify  data  into distinct classses based on the  features. I will try to  find best classification model using the given features and alternative classification algorithms .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this study, a POI (person of interest) is operationally defined as a person  who:\n",
    "* was  indicted\n",
    "* settled without admitting guilt\n",
    "* testified  in exchange for immunity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q1) How many data points (people) are in the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of employees:  146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q2) How many POIs are there in dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of poi's: 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q3) For each person, how many features are available?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There  exist 21  features in the dataset.  Financial features: 14, Email features: 6, POI label: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* financial features: ['salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', 'director_fees'] (all units are in US dollars)\n",
    "* email features: ['to_messages', 'email_address', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi'] (units are generally number of emails messages; notable exception is ‘email_address’, which is a text string)\n",
    "* POI label: [‘poi’] (boolean, represented as integer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q4)  Which fields include missing values in the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Missing fields and corresponding frequencies are given as below:\n",
    "* 'salary': 51 \n",
    "* 'to_messages': 60\n",
    "* 'deferral_payments': 107\n",
    "* 'total_payments': 21\n",
    "* 'long_term_incentive': 80 \n",
    "* 'loan_advances': 142 \n",
    "* 'bonus': 64\n",
    "* 'restricted_stock': 36 \n",
    "* 'restricted_stock_deferred': 128 \n",
    "* 'total_stock_value': 20\n",
    "* 'shared_receipt_with_poi': 60 \n",
    "* 'from_poi_to_this_person': 60\n",
    "* 'exercised_stock_options': 44\n",
    "* 'from_messages': 60\n",
    "* 'other': 53 \n",
    "* 'from_this_person_to_poi': 60\n",
    "* 'deferred_income': 97 \n",
    "* 'expenses': 51\n",
    "* 'email_address': 35 \n",
    "* 'director_fees': 129"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing features  are cleaned by the  feature_format function . Therefore, I did not make any addditional work to change the  feature_format function. While creating new features, I have  excluded  NaN fields in the dataset from division operations.\n",
    "We can also imput average value of the field for each missing data as a rule of thumb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q5)  What are the outliers in the dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outliers are 'TOTAL' and  'THE TRAVEL AGENCY IN THE PARK' .  The TOTAL is a spreadsheet quirk and THE TRAVEL AGENCY IN THE PARK is an important element but not a real person we are interested. So, Both of them were cleaned.  LAY KENNETH L and  SKILLING JEFFREY K are also outliers due to their  huge salaries and bonuses but  they are valid  data points. Therefore, I kept them in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q1)  What features did you end up using in your POI identifier, and what selection process did you use to pick them? \n",
    "To select features, I have used SelectKBest feature selection method.  This method  is a Univariate feature selection method works by selecting the best features based on univariate statistical tests. I have used all the features other than 'email_adress'  in  SKB. \n",
    "To find the best number of features (k) value, I have fed  all k alternatives (1-23) into the GridSearchCV  as a parameter. Therefore, for each algorithm , the best number of features were selected in the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q2)  Did you have to do any scaling? Why or why not?\n",
    "I have used scaling (minmax scaler) in the pipeline because  some of my algorithms (eg: KNeighborsClassifier) are distance based algorithms. To use these algorithms effectively, I have to use  scaling.\n",
    "I have used scaling  in the pipeline because if I do scaling before pipeline, I would be using information in the training data to perform scaling on the test data .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q3) As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it.\n",
    "I have created 3 new features to be used in the model:\n",
    "* from_this_person_fraction: This is the fraction of email messages a person sent to POIs.  It shows the relationship between POIs.\n",
    "* from_poi_to_this_person_fraction: This is the fraction of email messages a person recieved from POIs.  This is also an important  factor to find the relationship  between POIs.\n",
    "\n",
    "The rationale behind selecting these  features is that I hypothesize that POIs will have significantly higher  interaction than POI - non-POIs and Non-POIs have among each other.\n",
    "I have also created  one more feature:\n",
    "* salary/bonus ratio:  The rationale behind selecting this feature is that POIs may get significantly higher  bonus than the other people performing similar duties in the company.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## q4) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.\n",
    "\n",
    "I have used  SelectKBest Method for each method using pipeline and GridSearchCV.  For each method, all alternatives have been  tested by the GridSearchCV. I have used 3 algorithms: NaiveBayes, KNeighbor , and DecisionTree.\n",
    "* NaiveBayes: 5 features have been used (k: 5) in the best model. The features, their scores, and p values are given as below:  \n",
    " ('salary', '18.29', '0.0000'),   \n",
    " ('bonus', '20.79', '0.0000'),   \n",
    " ('total_stock_value', '24.18', '0.0000'),   \n",
    " ('exercised_stock_options', '24.82', '0.0000'),   \n",
    " ('from_this_person_fraction', '16.41', '0.0001') \n",
    "   \n",
    "     \n",
    " \n",
    " * KNeighborsClassifier: 3 features have been used (k: 3) in the best model. The features, their scores and p values are given as below:  \n",
    " ('bonus', '20.79', '0.0000'),  \n",
    " ('total_stock_value', '24.18', '0.0000'),  \n",
    " ('exercised_stock_options', '24.82', '0.0000') \n",
    " \n",
    " * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
